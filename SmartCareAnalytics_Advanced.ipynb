{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2101c483",
   "metadata": {},
   "source": [
    "# SmartCare Analytics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb21891",
   "metadata": {},
   "source": [
    " Extraction & Mod√®les de Pr√©vision\n",
    "Extraction du fichier SLP-CHF2012.pdf, construction base de donn√©es et pr√©visions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922da91e",
   "metadata": {},
   "source": [
    "## 1. Import des biblioth√®ques n√©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber tabula-py pandas numpy scikit-learn statsmodels prophet tensorflow matplotlib seaborn openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2577709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (d√©commenter si n√©cessaire)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import sqlite3\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PDF extraction\n",
    "import pdfplumber\n",
    "try:\n",
    "    import tabula\n",
    "except ImportError:\n",
    "    tabula = None\n",
    "\n",
    "# Machine Learning & Time Series\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "except ImportError:\n",
    "    sm = None\n",
    "    ARIMA = None\n",
    "\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except ImportError:\n",
    "    Prophet = None\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "except ImportError:\n",
    "    keras = None\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Toutes les biblioth√®ques ont √©t√© import√©es avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce57ef",
   "metadata": {},
   "source": [
    "## 2. Configuration et chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "WORKSPACE_DIR = Path.cwd()\n",
    "FICHIER_DIR = WORKSPACE_DIR / \"fichier\"\n",
    "PDF_FILE = FICHIER_DIR / \"SLP-CHF2012.pdf\"\n",
    "OUTPUT_DIR = WORKSPACE_DIR / \"output\"\n",
    "DB_FILE = OUTPUT_DIR / \"smartcare.db\"\n",
    "CSV_FILE = OUTPUT_DIR / \"smartcare_data.csv\"\n",
    "MODELS_DIR = OUTPUT_DIR / \"models\"\n",
    "\n",
    "# Cr√©er les r√©pertoires s'ils n'existent pas\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# V√©rifier l'existence du fichier PDF\n",
    "if PDF_FILE.exists():\n",
    "    logger.info(f\"‚úì PDF trouv√© : {PDF_FILE}\")\n",
    "    pdf_exists = True\n",
    "else:\n",
    "    logger.warning(f\"‚úó PDF non trouv√© : {PDF_FILE}\")\n",
    "    pdf_exists = False\n",
    "\n",
    "print(f\"R√©pertoire de travail : {WORKSPACE_DIR}\")\n",
    "print(f\"Base de donn√©es : {DB_FILE}\")\n",
    "print(f\"Fichier PDF : {PDF_FILE} (Existe: {pdf_exists})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ba4bd",
   "metadata": {},
   "source": [
    "## 3. Charger et inspecter le fichier PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca217929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_pdf(pdf_path):\n",
    "    \"\"\"Inspecte les m√©tadonn√©es et structure du PDF\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            print(f\"=== INSPECTION DU PDF ===\")\n",
    "            print(f\"Nombre de pages : {len(pdf.pages)}\")\n",
    "            \n",
    "            # M√©tadonn√©es\n",
    "            if pdf.metadata:\n",
    "                print(f\"\\nM√©tadonn√©es :\")\n",
    "                for key, value in pdf.metadata.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            \n",
    "            # Aper√ßu des premi√®res pages\n",
    "            print(f\"\\n=== APER√áU DES PREMI√àRES PAGES ===\")\n",
    "            for i in range(min(3, len(pdf.pages))):\n",
    "                page = pdf.pages[i]\n",
    "                print(f\"\\nPage {i+1}:\")\n",
    "                print(f\"  Texte brut (premiers 200 caract√®res):\")\n",
    "                text = page.extract_text()[:200] if page.extract_text() else \"Aucun texte\"\n",
    "                print(f\"  {text}...\")\n",
    "                \n",
    "                tables = page.extract_tables()\n",
    "                print(f\"  Nombre de tableaux : {len(tables) if tables else 0}\")\n",
    "                \n",
    "                return len(pdf.pages)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'inspection du PDF : {e}\")\n",
    "        return None\n",
    "\n",
    "# Inspecter le PDF\n",
    "if pdf_exists:\n",
    "    num_pages = inspect_pdf(PDF_FILE)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PDF non disponible pour l'inspection\")\n",
    "    num_pages = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cce4ed",
   "metadata": {},
   "source": [
    "## 4. Extraction du texte et des tableaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843dd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_tables(pdf_path):\n",
    "    \"\"\"Extrait le texte et les tableaux du PDF\"\"\"\n",
    "    all_text = []\n",
    "    all_tables = []\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            logger.info(f\"Extraction de {len(pdf.pages)} pages...\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                # Extraction du texte\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text.append({\n",
    "                        'page': page_num,\n",
    "                        'text': text\n",
    "                    })\n",
    "                \n",
    "                # Extraction des tableaux\n",
    "                tables = page.extract_tables()\n",
    "                if tables:\n",
    "                    for table_idx, table in enumerate(tables):\n",
    "                        df = pd.DataFrame(table)\n",
    "                        df['page'] = page_num\n",
    "                        df['table_id'] = f\"P{page_num}_T{table_idx+1}\"\n",
    "                        all_tables.append(df)\n",
    "                \n",
    "                if page_num % 10 == 0:\n",
    "                    logger.info(f\"  {page_num} pages trait√©es...\")\n",
    "        \n",
    "        logger.info(f\"‚úì Extraction termin√©e : {len(all_text)} pages, {len(all_tables)} tableaux\")\n",
    "        return all_text, all_tables\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'extraction : {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Extraction\n",
    "if pdf_exists:\n",
    "    extracted_text, extracted_tables = extract_text_and_tables(PDF_FILE)\n",
    "    print(f\"\\n‚úì Texte extrait : {len(extracted_text)} pages\")\n",
    "    print(f\"‚úì Tableaux extraits : {len(extracted_tables)} tableaux\")\n",
    "    \n",
    "    if extracted_tables:\n",
    "        print(f\"\\nAper√ßu du premier tableau :\")\n",
    "        print(extracted_tables[0].head())\n",
    "else:\n",
    "    extracted_text = []\n",
    "    extracted_tables = []\n",
    "    print(\"‚ö†Ô∏è Extraction impossible sans le PDF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadeed7",
   "metadata": {},
   "source": [
    "## 5. Nettoyage et pr√©traitement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd41f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric_value(val):\n",
    "    \"\"\"Nettoie et convertit les valeurs num√©riques\"\"\"\n",
    "    if pd.isna(val) or val == '' or val is None:\n",
    "        return np.nan\n",
    "    \n",
    "    val_str = str(val).strip()\n",
    "    \n",
    "    # Supprimer les caract√®res non num√©riques sauf point et tiret\n",
    "    val_str = re.sub(r'[^\\d\\.,\\-]', '', val_str)\n",
    "    \n",
    "    # Remplacer virgule par point pour standardiser\n",
    "    val_str = val_str.replace(',', '.')\n",
    "    \n",
    "    try:\n",
    "        return float(val_str) if val_str else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"Nettoie un DataFrame extract√© du PDF\"\"\"\n",
    "    # Supprimer les lignes compl√®tement vides\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    # Supprimer les colonnes enti√®rement vides\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Nettoyer les en-t√™tes\n",
    "    df.columns = [str(col).strip() for col in df.columns]\n",
    "    \n",
    "    # Supprimer les doublons de lignes\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Nettoyer le texte dans toutes les colonnes\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].apply(lambda x: str(x).strip() if pd.notna(x) else '')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def combine_tables(tables_list):\n",
    "    \"\"\"Combine plusieurs tableaux extraits\"\"\"\n",
    "    if not tables_list:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    cleaned_tables = [clean_dataframe(t) for t in tables_list]\n",
    "    \n",
    "    # Fusionner tous les tableaux\n",
    "    combined = pd.concat(cleaned_tables, ignore_index=True, sort=False)\n",
    "    \n",
    "    logger.info(f\"Tableaux combin√©s : {len(combined)} lignes, {len(combined.columns)} colonnes\")\n",
    "    return combined\n",
    "\n",
    "# Nettoyage\n",
    "if extracted_tables:\n",
    "    df_clean = combine_tables(extracted_tables)\n",
    "    print(f\"‚úì DataFrames combin√©s : {df_clean.shape[0]} lignes √ó {df_clean.shape[1]} colonnes\")\n",
    "    print(f\"\\nAper√ßu :\")\n",
    "    print(df_clean.head(10))\n",
    "else:\n",
    "    df_clean = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è Aucun tableau √† nettoyer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d0e58",
   "metadata": {},
   "source": [
    "## 6. Validation et gestion des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae973268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_fill_missing_values(df):\n",
    "    \"\"\"Valide les donn√©es et g√®re les valeurs manquantes\"\"\"\n",
    "    print(\"=== VALIDATION ET GESTION DES VALEURS MANQUANTES ===\\n\")\n",
    "    \n",
    "    # Rapport sur les valeurs manquantes\n",
    "    missing_report = pd.DataFrame({\n",
    "        'Colonne': df.columns,\n",
    "        'Manquantes': df.isnull().sum(),\n",
    "        'Pourcentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    }).sort_values('Pourcentage', ascending=False)\n",
    "    \n",
    "    print(\"Valeurs manquantes par colonne :\")\n",
    "    print(missing_report)\n",
    "    \n",
    "    # Supprimer les colonnes avec plus de 80% de valeurs manquantes\n",
    "    cols_to_drop = missing_report[missing_report['Pourcentage'] > 80]['Colonne'].tolist()\n",
    "    if cols_to_drop:\n",
    "        print(f\"\\nColonnes supprim√©es (>80% manquantes) : {cols_to_drop}\")\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Pour les colonnes num√©riques, imputer par la m√©diane\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            median_val = df[col].median()\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    # Pour les colonnes texte, imputer par 'Inconnu'\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna('Inconnu', inplace=True)\n",
    "    \n",
    "    print(f\"\\n‚úì Validation compl√©t√©e. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Application\n",
    "if not df_clean.empty:\n",
    "    df_clean = validate_and_fill_missing_values(df_clean)\n",
    "    print(\"\\n‚úì Donn√©es valid√©es et compl√©t√©es\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e5d59",
   "metadata": {},
   "source": [
    "## 7. Stocker dans une base de donn√©es SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdc3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_sqlite(df, db_path, table_name='smartcare_data'):\n",
    "    \"\"\"Sauvegarde le DataFrame dans une base de donn√©es SQLite\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        conn.commit()\n",
    "        \n",
    "        # V√©rifier l'insertion\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        \n",
    "        logger.info(f\"‚úì {count} lignes sauvegard√©es dans {table_name}\")\n",
    "        \n",
    "        # Afficher le sch√©ma\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        schema = cursor.fetchall()\n",
    "        print(f\"\\nSch√©ma de la table '{table_name}' :\")\n",
    "        for col_id, name, type_, notnull, default, pk in schema:\n",
    "            print(f\"  - {name}: {type_}\")\n",
    "        \n",
    "        conn.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la sauvegarde SQLite : {e}\")\n",
    "        return False\n",
    "\n",
    "def save_to_csv(df, csv_path):\n",
    "    \"\"\"Sauvegarde le DataFrame en CSV\"\"\"\n",
    "    try:\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig', sep=',')\n",
    "        logger.info(f\"‚úì Donn√©es sauvegard√©es en CSV : {csv_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la sauvegarde CSV : {e}\")\n",
    "        return False\n",
    "\n",
    "# Sauvegarde\n",
    "if not df_clean.empty:\n",
    "    save_to_sqlite(df_clean, DB_FILE, 'smartcare_data')\n",
    "    save_to_csv(df_clean, CSV_FILE)\n",
    "    print(f\"\\n‚úì Fichiers de sortie cr√©√©s :\")\n",
    "    print(f\"  - Base de donn√©es : {DB_FILE}\")\n",
    "    print(f\"  - CSV : {CSV_FILE}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucune donn√©e √† sauvegarder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b7716",
   "metadata": {},
   "source": [
    "## 8. Analyse exploratoire et visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploratory_analysis(df):\n",
    "    \"\"\"Analyse exploratoire des donn√©es\"\"\"\n",
    "    print(\"=== ANALYSE EXPLORATOIRE ===\\n\")\n",
    "    \n",
    "    # Statistiques g√©n√©rales\n",
    "    print(f\"Shape : {df.shape}\")\n",
    "    print(f\"\\nTypes de donn√©es :\\n{df.dtypes}\\n\")\n",
    "    \n",
    "    print(\"Statistiques descriptives :\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Donn√©es num√©riques\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if numeric_cols:\n",
    "        print(f\"\\n\\nColonnes num√©riques : {numeric_cols}\")\n",
    "        \n",
    "        # Cr√©er des visualisations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle('Analyse Exploratoire des Donn√©es', fontsize=16)\n",
    "        \n",
    "        # Distribution des premi√®res colonnes num√©riques\n",
    "        for idx, col in enumerate(numeric_cols[:4]):\n",
    "            ax = axes[idx // 2, idx % 2]\n",
    "            df[col].hist(bins=30, ax=ax, edgecolor='black')\n",
    "            ax.set_title(f'Distribution : {col}')\n",
    "            ax.set_ylabel('Fr√©quence')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / 'exploratory_analysis.png', dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        logger.info(\"‚úì Graphique sauvegard√© : exploratory_analysis.png\")\n",
    "    \n",
    "    # Colonnes cat√©gories\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if categorical_cols and len(categorical_cols) > 0:\n",
    "        print(f\"\\n\\nColonnes cat√©gories : {categorical_cols[:5]}\")\n",
    "        for col in categorical_cols[:3]:\n",
    "            print(f\"\\n{col} - Top 5 valeurs :\")\n",
    "            print(df[col].value_counts().head())\n",
    "\n",
    "# Analyse\n",
    "if not df_clean.empty:\n",
    "    exploratory_analysis(df_clean)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas de donn√©es pour l'analyse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557708f",
   "metadata": {},
   "source": [
    "## 9. Pr√©paration des donn√©es pour la mod√©lisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des donn√©es synth√©tiques de d√©monstration si le PDF ne contient pas assez de donn√©es\n",
    "def create_synthetic_healthcare_data(n_samples=1000):\n",
    "    \"\"\"Cr√©e des donn√©es de sant√© synth√©tiques pour la d√©monstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    dates = pd.date_range(start='2020-01-01', periods=n_samples, freq='D')\n",
    "    \n",
    "    data = {\n",
    "        'date': dates,\n",
    "        'patient_id': np.random.randint(1000, 2000, n_samples),\n",
    "        'age': np.random.randint(18, 85, n_samples),\n",
    "        'frequence_cardiaque': np.random.normal(70, 10, n_samples),  # bpm\n",
    "        'tension_arterielle_sys': np.random.normal(120, 15, n_samples),  # mmHg\n",
    "        'tension_arterielle_dia': np.random.normal(80, 10, n_samples),\n",
    "        'glucose': np.random.normal(100, 20, n_samples),  # mg/dL\n",
    "        'cholesterol': np.random.normal(200, 40, n_samples),  # mg/dL\n",
    "        'diagnose': np.random.choice(['Sain', 'Hypertension', 'Diab√®te', 'CHF', 'Autre'], n_samples),\n",
    "        'traitement': np.random.choice(['Aucun', 'M√©dicament A', 'M√©dicament B', 'Intervention'], n_samples)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Pr√©paration des donn√©es\n",
    "if not df_clean.empty:\n",
    "    df_model = df_clean.copy()\n",
    "else:\n",
    "    # Cr√©er des donn√©es synth√©tiques\n",
    "    print(\"‚ö†Ô∏è Utilisation de donn√©es synth√©tiques pour la d√©monstration\")\n",
    "    df_model = create_synthetic_healthcare_data(500)\n",
    "    print(f\"Donn√©es synth√©tiques cr√©√©es : {df_model.shape}\")\n",
    "\n",
    "# Sauvegarder les donn√©es mod√®le\n",
    "if df_model.empty is False:\n",
    "    df_model.to_csv(OUTPUT_DIR / 'smartcare_model_data.csv', index=False)\n",
    "    logger.info(\"‚úì Donn√©es de mod√©lisation sauvegard√©es\")\n",
    "\n",
    "print(f\"\\n‚úì Donn√©es pr√™tes pour la mod√©lisation : {df_model.shape}\")\n",
    "print(df_model.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d552d6",
   "metadata": {},
   "source": [
    "## 10. Mod√®les de pr√©vision - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eecc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_models(df_model):\n",
    "    \"\"\"Entra√Æne des mod√®les de r√©gression pour pr√©dire la fr√©quence cardiaque\"\"\"\n",
    "    print(\"=== MOD√àLES DE R√âGRESSION ===\\n\")\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    numeric_cols = df_model.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if 'frequence_cardiaque' not in numeric_cols:\n",
    "        # Si non disponible, utiliser la premi√®re colonne num√©rique comme cible\n",
    "        target_col = numeric_cols[0] if numeric_cols else None\n",
    "        if target_col is None:\n",
    "            print(\"‚ö†Ô∏è Aucune colonne num√©rique trouv√©e\")\n",
    "            return {}\n",
    "    else:\n",
    "        target_col = 'frequence_cardiaque'\n",
    "    \n",
    "    # S√©lectionner les features (colonnes num√©riques sauf la cible)\n",
    "    feature_cols = [col for col in numeric_cols if col != target_col]\n",
    "    \n",
    "    if len(feature_cols) < 2:\n",
    "        print(\"‚ö†Ô∏è Pas assez de features num√©riques\")\n",
    "        return {}\n",
    "    \n",
    "    X = df_model[feature_cols].fillna(0)\n",
    "    y = df_model[target_col].fillna(0)\n",
    "    \n",
    "    # Diviser train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normaliser\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    models = {}\n",
    "    results = []\n",
    "    \n",
    "    # 1. R√©gression Lin√©aire\n",
    "    print(\"1. R√©gression Lin√©aire...\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    y_pred_lr = lr.predict(X_test_scaled)\n",
    "    \n",
    "    mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "    rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "    r2_lr = r2_score(y_test, y_pred_lr)\n",
    "    \n",
    "    models['Linear Regression'] = lr\n",
    "    results.append({'Model': 'Linear Regression', 'MAE': mae_lr, 'RMSE': rmse_lr, 'R¬≤': r2_lr})\n",
    "    print(f\"   MAE: {mae_lr:.4f}, RMSE: {rmse_lr:.4f}, R¬≤: {r2_lr:.4f}\")\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"2. Random Forest...\")\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    \n",
    "    mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "    rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "    r2_rf = r2_score(y_test, y_pred_rf)\n",
    "    \n",
    "    models['Random Forest'] = rf\n",
    "    results.append({'Model': 'Random Forest', 'MAE': mae_rf, 'RMSE': rmse_rf, 'R¬≤': r2_rf})\n",
    "    print(f\"   MAE: {mae_rf:.4f}, RMSE: {rmse_rf:.4f}, R¬≤: {r2_rf:.4f}\")\n",
    "    \n",
    "    # 3. Gradient Boosting\n",
    "    print(\"3. Gradient Boosting...\")\n",
    "    gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred_gb = gb.predict(X_test)\n",
    "    \n",
    "    mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "    rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "    r2_gb = r2_score(y_test, y_pred_gb)\n",
    "    \n",
    "    models['Gradient Boosting'] = gb\n",
    "    results.append({'Model': 'Gradient Boosting', 'MAE': mae_gb, 'RMSE': rmse_gb, 'R¬≤': r2_gb})\n",
    "    print(f\"   MAE: {mae_gb:.4f}, RMSE: {rmse_gb:.4f}, R¬≤: {r2_gb:.4f}\")\n",
    "    \n",
    "    # R√©sum√©\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.suptitle('Comparaison des Mod√®les de R√©gression', fontsize=14)\n",
    "    \n",
    "    predictions = [y_pred_lr, y_pred_rf, y_pred_gb]\n",
    "    model_names = ['Linear Regression', 'Random Forest', 'Gradient Boosting']\n",
    "    \n",
    "    for ax, pred, name in zip(axes, predictions, model_names):\n",
    "        ax.scatter(y_test, pred, alpha=0.5)\n",
    "        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        ax.set_xlabel('Valeurs R√©elles')\n",
    "        ax.set_ylabel('Pr√©dictions')\n",
    "        ax.set_title(name)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'regression_models.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return models, results_df, scaler, feature_cols\n",
    "\n",
    "# Entra√Ænement\n",
    "regression_models, regression_results, scaler, feature_cols = train_regression_models(df_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18cf0b9",
   "metadata": {},
   "source": [
    "## 11. Mod√®les de pr√©vision - S√©ries Temporelles (ARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_arima_model(df_model):\n",
    "    \"\"\"Entra√Æne un mod√®le ARIMA pour les s√©ries temporelles\"\"\"\n",
    "    print(\"=== MOD√àLE ARIMA - S√âRIES TEMPORELLES ===\\n\")\n",
    "    \n",
    "    if ARIMA is None:\n",
    "        print(\"‚ö†Ô∏è statsmodels non install√©. Installation recommand√©e : pip install statsmodels\")\n",
    "        return None, None\n",
    "    \n",
    "    # Pr√©parer les donn√©es de s√©rie temporelle\n",
    "    numeric_cols = df_model.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if not numeric_cols:\n",
    "        print(\"‚ö†Ô∏è Aucune colonne num√©rique\")\n",
    "        return None, None\n",
    "    \n",
    "    # Utiliser la premi√®re colonne num√©rique\n",
    "    ts_data = df_model[numeric_cols[0]].dropna()\n",
    "    \n",
    "    if len(ts_data) < 50:\n",
    "        print(\"‚ö†Ô∏è Pas assez de donn√©es pour ARIMA (min 50)\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"S√©rie temporelle : {len(ts_data)} points de donn√©es\")\n",
    "    \n",
    "    try:\n",
    "        # Diviser train/test\n",
    "        train_size = int(len(ts_data) * 0.8)\n",
    "        train, test = ts_data[:train_size], ts_data[train_size:]\n",
    "        \n",
    "        # Entra√Æner ARIMA(1,1,1)\n",
    "        print(\"Entra√Ænement d'ARIMA(1,1,1)...\")\n",
    "        model_arima = ARIMA(train, order=(1, 1, 1))\n",
    "        results_arima = model_arima.fit()\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        predictions = results_arima.forecast(steps=len(test))\n",
    "        \n",
    "        mae = mean_absolute_error(test, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "        \n",
    "        print(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        # Visualisation\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        ax.plot(range(len(train)), train, label='Entra√Ænement', color='blue')\n",
    "        ax.plot(range(len(train), len(train) + len(test)), test, label='Test R√©el', color='green')\n",
    "        ax.plot(range(len(train), len(train) + len(predictions)), predictions, label='Pr√©dictions ARIMA', color='red', linestyle='--')\n",
    "        ax.set_title('Mod√®le ARIMA - Pr√©dictions vs R√©alit√©')\n",
    "        ax.set_xlabel('Temps')\n",
    "        ax.set_ylabel('Valeur')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / 'arima_forecast.png', dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return results_arima, {'MAE': mae, 'RMSE': rmse}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur ARIMA : {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Entra√Ænement ARIMA\n",
    "arima_model, arima_results = train_arima_model(df_model)\n",
    "if arima_results:\n",
    "    print(f\"‚úì ARIMA mod√®le cr√©√© avec succ√®s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588166a",
   "metadata": {},
   "source": [
    "## 12. Mod√®les de pr√©vision - Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ac96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prophet_model(df_model):\n",
    "    \"\"\"Entra√Æne un mod√®le Prophet pour les pr√©visions\"\"\"\n",
    "    print(\"=== MOD√àLE PROPHET ===\\n\")\n",
    "    \n",
    "    if Prophet is None:\n",
    "        print(\"‚ö†Ô∏è Prophet non install√©. Installation recommand√©e : pip install prophet\")\n",
    "        return None, None\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    if 'date' in df_model.columns:\n",
    "        df_prophet = df_model[['date']].copy()\n",
    "    else:\n",
    "        df_prophet = pd.DataFrame({'date': pd.date_range(start='2020-01-01', periods=len(df_model), freq='D')})\n",
    "    \n",
    "    numeric_cols = df_model.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        print(\"‚ö†Ô∏è Aucune colonne num√©rique\")\n",
    "        return None, None\n",
    "    \n",
    "    y_col = numeric_cols[0]\n",
    "    df_prophet['y'] = df_model[y_col].values\n",
    "    \n",
    "    # Renommer pour Prophet\n",
    "    df_prophet.columns = ['ds', 'y']\n",
    "    df_prophet = df_prophet.dropna()\n",
    "    \n",
    "    if len(df_prophet) < 50:\n",
    "        print(\"‚ö†Ô∏è Pas assez de donn√©es pour Prophet\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Entra√Ænement de Prophet sur {len(df_prophet)} points...\")\n",
    "        \n",
    "        model_prophet = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)\n",
    "        model_prophet.fit(df_prophet)\n",
    "        \n",
    "        # Pr√©visions futures\n",
    "        future = model_prophet.make_future_dataframe(periods=30)\n",
    "        forecast = model_prophet.predict(future)\n",
    "        \n",
    "        # Visualisation\n",
    "        fig = model_prophet.plot(forecast)\n",
    "        plt.title('Pr√©visions Prophet')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / 'prophet_forecast.png', dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Composants\n",
    "        fig = model_prophet.plot_components(forecast)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / 'prophet_components.png', dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return model_prophet, forecast\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur Prophet : {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Entra√Ænement Prophet\n",
    "prophet_model, prophet_forecast = train_prophet_model(df_model)\n",
    "if prophet_model:\n",
    "    print(f\"‚úì Prophet mod√®le cr√©√© avec succ√®s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc262c",
   "metadata": {},
   "source": [
    "## 13. Sauvegarde des mod√®les et pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(models_dict, scaler, feature_cols):\n",
    "    \"\"\"Sauvegarde les mod√®les entra√Æn√©s\"\"\"\n",
    "    print(\"=== SAUVEGARDE DES MOD√àLES ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Sauvegarder les mod√®les\n",
    "        for model_name, model in models_dict.items():\n",
    "            model_path = MODELS_DIR / f\"{model_name.lower().replace(' ', '_')}.pkl\"\n",
    "            joblib.dump(model, model_path)\n",
    "            logger.info(f\"‚úì {model_name} sauvegard√© : {model_path}\")\n",
    "        \n",
    "        # Sauvegarder le scaler\n",
    "        scaler_path = MODELS_DIR / \"scaler.pkl\"\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        logger.info(f\"‚úì Scaler sauvegard√© : {scaler_path}\")\n",
    "        \n",
    "        # Sauvegarder les features utilis√©es\n",
    "        config = {\n",
    "            'feature_cols': feature_cols,\n",
    "            'n_features': len(feature_cols),\n",
    "            'model_names': list(models_dict.keys()),\n",
    "            'created_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        config_path = MODELS_DIR / \"config.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        logger.info(f\"‚úì Configuration sauvegard√©e : {config_path}\")\n",
    "        \n",
    "        print(\"\\n‚úì Tous les mod√®les ont √©t√© sauvegard√©s\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la sauvegarde : {e}\")\n",
    "        return False\n",
    "\n",
    "# Sauvegarde\n",
    "if regression_models:\n",
    "    save_models(regression_models, scaler, feature_cols)\n",
    "\n",
    "# Cr√©er un script d'inf√©rence\n",
    "inference_script = '''#!/usr/bin/env python\n",
    "\"\"\"Script d'inf√©rence - Utilisation des mod√®les entra√Æn√©s\"\"\"\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Charger les mod√®les et config\n",
    "MODELS_DIR = Path(__file__).parent / \"models\"\n",
    "\n",
    "config = json.load(open(MODELS_DIR / \"config.json\"))\n",
    "scaler = joblib.load(MODELS_DIR / \"scaler.pkl\")\n",
    "\n",
    "def predict(X_new):\n",
    "    \"\"\"Effectue une pr√©diction avec les mod√®les\"\"\"\n",
    "    # Charger les mod√®les\n",
    "    models = {}\n",
    "    for model_name in config['model_names']:\n",
    "        model_path = MODELS_DIR / f\"{model_name.lower().replace(' ', '_')}.pkl\"\n",
    "        models[model_name] = joblib.load(model_path)\n",
    "    \n",
    "    # S√©lectionner les features\n",
    "    X_new = X_new[config['feature_cols']]\n",
    "    \n",
    "    # Normaliser\n",
    "    X_scaled = scaler.transform(X_new)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    predictions = {}\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            pred = model.predict(X_scaled if 'Linear' in model_name else X_new)\n",
    "            predictions[model_name] = pred\n",
    "        except:\n",
    "            predictions[model_name] = None\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemple d'utilisation\n",
    "    print(\"Script d'inf√©rence - SmartCare Analytics\")\n",
    "    print(f\"Mod√®les disponibles : {config['model_names']}\")\n",
    "    print(f\"Features attendues : {config['feature_cols']}\")\n",
    "'''\n",
    "\n",
    "with open(MODELS_DIR / \"inference.py\", 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "logger.info(\"‚úì Script d'inf√©rence cr√©√©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc6fbbd",
   "metadata": {},
   "source": [
    "## 14. R√©sum√© et rapport final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report():\n",
    "    \"\"\"G√©n√®re un rapport final complet\"\"\"\n",
    "    report = f\"\"\"\n",
    "{'='*80}\n",
    "RAPPORT FINAL - SMARTCARE ANALYTICS\n",
    "{'='*80}\n",
    "\n",
    "üìä EXTRACTION DES DONN√âES\n",
    "  - Fichier PDF : {PDF_FILE}\n",
    "  - Texte extrait : {len(extracted_text)} pages\n",
    "  - Tableaux extraits : {len(extracted_tables)} tableaux\n",
    "  \n",
    "üìà BASE DE DONN√âES\n",
    "  - Format : SQLite et CSV\n",
    "  - Emplacement : {OUTPUT_DIR}\n",
    "  - Lignes : {df_clean.shape[0] if not df_clean.empty else 'N/A'}\n",
    "  - Colonnes : {df_clean.shape[1] if not df_clean.empty else 'N/A'}\n",
    "\n",
    "ü§ñ MOD√àLES ENTRA√éN√âS\n",
    "  \n",
    "  1. R√âGRESSION\n",
    "\"\"\"\n",
    "    \n",
    "    if not regression_results.empty:\n",
    "        report += f\"\\n     {regression_results.to_string(index=False)}\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "  \n",
    "  2. S√âRIES TEMPORELLES\n",
    "\"\"\"\n",
    "    \n",
    "    if arima_results:\n",
    "        report += f\"\\n     ARIMA : MAE={arima_results.get('MAE', 'N/A')}, RMSE={arima_results.get('RMSE', 'N/A')}\"\n",
    "    else:\n",
    "        report += \"\\n     ARIMA : Non disponible\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "  \n",
    "  3. PROPHET\n",
    "\"\"\"\n",
    "    if prophet_model:\n",
    "        report += \"\\n     Prophet : Entra√Æn√© avec succ√®s\"\n",
    "    else:\n",
    "        report += \"\\n     Prophet : Non disponible\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "üìÅ FICHIERS G√âN√âR√âS\n",
    "  - Base de donn√©es SQLite : {DB_FILE}\n",
    "  - Donn√©es CSV : {CSV_FILE}\n",
    "  - Donn√©es mod√®le : {OUTPUT_DIR / 'smartcare_model_data.csv'}\n",
    "  - Mod√®les sauvegard√©s : {MODELS_DIR}\n",
    "  - Visualisations :\n",
    "    * exploratory_analysis.png\n",
    "    * regression_models.png\n",
    "    * arima_forecast.png\n",
    "    * prophet_forecast.png\n",
    "    * prophet_components.png\n",
    "\n",
    "‚úÖ √âTAPES COMPL√âT√âES\n",
    "  ‚úì Extraction du PDF\n",
    "  ‚úì Nettoyage des donn√©es\n",
    "  ‚úì Validation et imputation\n",
    "  ‚úì Cr√©ation de la base de donn√©es\n",
    "  ‚úì Analyse exploratoire\n",
    "  ‚úì Mod√®les de r√©gression\n",
    "  ‚úì Mod√®les de s√©ries temporelles\n",
    "  ‚úì Sauvegarde des mod√®les\n",
    "\n",
    "üöÄ PROCHAINES √âTAPES\n",
    "  1. Optimiser les hyperparam√®tres des mod√®les\n",
    "  2. Impl√©menter des mod√®les plus avanc√©s (LSTM, Transformer)\n",
    "  3. Effectuer des pr√©dictions sur de nouvelles donn√©es\n",
    "  4. D√©ployer les mod√®les en production\n",
    "  5. Mettre en place un syst√®me de monitoring\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    # Sauvegarder le rapport\n",
    "    report_path = OUTPUT_DIR / \"RAPPORT_FINAL.txt\"\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    logger.info(f\"‚úì Rapport sauvegard√© : {report_path}\")\n",
    "\n",
    "# G√©n√©rer le rapport\n",
    "generate_final_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® PIPELINE COMPLET TERMIN√â AVEC SUCC√àS ‚ú®\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
